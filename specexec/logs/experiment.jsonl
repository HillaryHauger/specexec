{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ", "model_1": "TheBloke/Llama-2-7B-Chat-GPTQ", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": null, "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-11 15:33:31", "date": "250311", "hostname": "gpu12", "commit": "none", "offload": true, "device": "GeForce GTX 1080 Ti", "device_size": 2}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": null, "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-11 17:36:04", "date": "250311", "hostname": "gpu07", "commit": "none", "offload": false, "device": "GeForce GTX 1080 Ti"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": 10, "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-11 17:38:00", "date": "250311", "hostname": "gpu07", "commit": "none", "offload": false, "device": "GeForce GTX 1080 Ti"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 09:12:57", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 09:23:30", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 09:28:23", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 09:36:50", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 09:43:26", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 15:10:42", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 15:14:43", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 15:18:22", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 15:24:12", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 15:25:56", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 15:27:54", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 15:30:40", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 15:37:08", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 15:51:41", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "summary", "run": 0, "max_n_beams": 128, "max_beam_len": 32, "branching": null, "max_budget": 10, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 1.0, "draft_temperature": null, "top_p": 1.0, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590", "prompt_len": 146, "prompt_text": "What is a Dyson Sphere? [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 8, "new_tokens": 34, "tree_h": 5.0, "tree_w": 4, "tree_size": 10, "t0": 0.135, "t1": 1.23, "tft": 6.8744, "input_0": 23, "input_1": 11, "min_CLP": -2.97, "draft_iters": 6.0, "mem_use": 4.6, "gen_rate": 4.2, "speed": 3.11, "text": "\\nA Dyson Sphere is a hypothetic"}
{"msg_type": "summary", "run": 1, "max_n_beams": 128, "max_beam_len": 32, "branching": null, "max_budget": 10, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 1.0, "draft_temperature": null, "top_p": 1.0, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590", "prompt_len": 436, "prompt_text": "heck your facts for me. [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 7, "new_tokens": 32, "tree_h": 7.1, "tree_w": 2, "tree_size": 10, "t0": 0.1829, "t1": 1.2229, "tft": 1.3834, "input_0": 27, "input_1": 11, "min_CLP": -2.8, "draft_iters": 8.1, "mem_use": 4.76, "gen_rate": 4.6, "speed": 3.25, "text": "My apologies, you are correct. T"}
{"msg_type": "summary", "run": 2, "max_n_beams": 128, "max_beam_len": 32, "branching": null, "max_budget": 10, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 1.0, "draft_temperature": null, "top_p": 1.0, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590", "prompt_len": 597, "prompt_text": ". Factorio\n3. Stellaris [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 6, "new_tokens": 33, "tree_h": 5.3, "tree_w": 3, "tree_size": 10, "t0": 0.1367, "t1": 1.2033, "tft": 1.4975, "input_0": 21, "input_1": 11, "min_CLP": -3.03, "draft_iters": 6.3, "mem_use": 4.86, "gen_rate": 5.5, "speed": 4.11, "text": "Sure, here are some general guid"}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": "10", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 15:53:08", "date": "250312", "hostname": "gpu17", "commit": "none", "offload": false, "device": "Tesla V100-PCIE-32GB"}
{"msg_type": "summary", "run": 0, "max_n_beams": 128, "max_beam_len": 32, "branching": null, "max_budget": 10, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 1.0, "draft_temperature": null, "top_p": 1.0, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590", "prompt_len": 146, "prompt_text": "What is a Dyson Sphere? [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 8, "new_tokens": 34, "tree_h": 5.0, "tree_w": 4, "tree_size": 10, "t0": 0.1375, "t1": 1.5512, "tft": 5.6924, "input_0": 23, "input_1": 11, "min_CLP": -2.97, "draft_iters": 6.0, "mem_use": 4.6, "gen_rate": 4.2, "speed": 2.51, "text": "\\nA Dyson Sphere is a hypothetic"}
{"msg_type": "summary", "run": 1, "max_n_beams": 128, "max_beam_len": 32, "branching": null, "max_budget": 10, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 1.0, "draft_temperature": null, "top_p": 1.0, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590", "prompt_len": 436, "prompt_text": "heck your facts for me. [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 7, "new_tokens": 32, "tree_h": 7.1, "tree_w": 2, "tree_size": 10, "t0": 0.1814, "t1": 1.38, "tft": 1.393, "input_0": 27, "input_1": 11, "min_CLP": -2.8, "draft_iters": 8.1, "mem_use": 4.76, "gen_rate": 4.6, "speed": 2.93, "text": "My apologies, you are correct. T"}
{"msg_type": "summary", "run": 2, "max_n_beams": 128, "max_beam_len": 32, "branching": null, "max_budget": 10, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 1.0, "draft_temperature": null, "top_p": 1.0, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590", "prompt_len": 597, "prompt_text": ". Factorio\n3. Stellaris [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 6, "new_tokens": 33, "tree_h": 5.3, "tree_w": 3, "tree_size": 10, "t0": 0.1483, "t1": 1.1983, "tft": 1.4888, "input_0": 21, "input_1": 11, "min_CLP": -3.03, "draft_iters": 6.3, "mem_use": 4.86, "gen_rate": 5.5, "speed": 4.08, "text": "Sure, here are some general guid"}
{"msg_type": "summary", "run": 3, "max_n_beams": 128, "max_beam_len": 32, "branching": null, "max_budget": 10, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 1.0, "draft_temperature": null, "top_p": 1.0, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590", "prompt_len": 181, "prompt_text": "a) when x is user input [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 10, "new_tokens": 35, "tree_h": 5.5, "tree_w": 3, "tree_size": 10, "t0": 0.14, "t1": 1.199, "tft": 1.3685, "input_0": 22, "input_1": 11, "min_CLP": -2.82, "draft_iters": 6.5, "mem_use": 4.61, "gen_rate": 3.5, "speed": 2.62, "text": "\"\\nI apologize, but I cannot cre"}
{"msg_type": "summary", "run": 4, "max_n_beams": 128, "max_beam_len": 32, "branching": null, "max_budget": 10, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 1.0, "draft_temperature": null, "top_p": 1.0, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590", "prompt_len": 148, "prompt_text": "elp me write my memoir? [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 7, "new_tokens": 36, "tree_h": 6.3, "tree_w": 3, "tree_size": 10, "t0": 0.1643, "t1": 1.1914, "tft": 1.3009, "input_0": 26, "input_1": 11, "min_CLP": -2.7, "draft_iters": 7.3, "mem_use": 4.6, "gen_rate": 5.1, "speed": 3.79, "text": "\"\\nOf course, I'd be happy to he"}
{"msg_type": "summary", "run": 5, "max_n_beams": 128, "max_beam_len": 32, "branching": null, "max_budget": 10, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 1.0, "draft_temperature": null, "top_p": 1.0, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590", "prompt_len": 632, "prompt_text": "contents of this video. [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 7, "new_tokens": 34, "tree_h": 8.0, "tree_w": 2, "tree_size": 10, "t0": 0.1943, "t1": 1.2143, "tft": 1.6161, "input_0": 26, "input_1": 11, "min_CLP": -2.67, "draft_iters": 9.0, "mem_use": 4.89, "gen_rate": 4.9, "speed": 3.45, "text": "Sure, here are some potential bu"}
{"msg_type": "summary", "run": 6, "max_n_beams": 128, "max_beam_len": 32, "branching": null, "max_budget": 10, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 1.0, "draft_temperature": null, "top_p": 1.0, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590", "prompt_len": 728, "prompt_text": "ions to enhance images? [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 8, "new_tokens": 42, "tree_h": 7.2, "tree_w": 2, "tree_size": 10, "t0": 0.1825, "t1": 1.2175, "tft": 1.6058, "input_0": 24, "input_1": 11, "min_CLP": -2.25, "draft_iters": 8.2, "mem_use": 4.97, "gen_rate": 5.2, "speed": 3.75, "text": "There are several other options "}
{"msg_type": "summary", "run": 7, "max_n_beams": 128, "max_beam_len": 32, "branching": null, "max_budget": 10, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 1.0, "draft_temperature": null, "top_p": 1.0, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590", "prompt_len": 1206, "prompt_text": "ge scale manufacturing. [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 7, "new_tokens": 32, "tree_h": 6.1, "tree_w": 3, "tree_size": 10, "t0": 0.1643, "t1": 1.2157, "tft": 1.7549, "input_0": 25, "input_1": 11, "min_CLP": -2.76, "draft_iters": 7.1, "mem_use": 5.53, "gen_rate": 4.6, "speed": 3.31, "text": "\"Great, let's dive into the stra"}
{"msg_type": "summary", "run": 8, "max_n_beams": 128, "max_beam_len": 32, "branching": null, "max_budget": 10, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 1.0, "draft_temperature": null, "top_p": 1.0, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590", "prompt_len": 699, "prompt_text": "a for all 50 US states. [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 10, "new_tokens": 33, "tree_h": 6.3, "tree_w": 2, "tree_size": 10, "t0": 0.166, "t1": 1.266, "tft": 1.6131, "input_0": 24, "input_1": 11, "min_CLP": -2.28, "draft_iters": 7.3, "mem_use": 4.96, "gen_rate": 3.3, "speed": 2.31, "text": "Certainly! Here is the legal sta"}
{"msg_type": "summary", "run": 9, "max_n_beams": 128, "max_beam_len": 32, "branching": null, "max_budget": 10, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 1.0, "draft_temperature": null, "top_p": 1.0, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590", "prompt_len": 562, "prompt_text": "that I could start now? [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 6, "new_tokens": 32, "tree_h": 5.7, "tree_w": 3, "tree_size": 10, "t0": 0.1467, "t1": 1.205, "tft": 1.5226, "input_0": 22, "input_1": 11, "min_CLP": -2.59, "draft_iters": 6.7, "mem_use": 4.84, "gen_rate": 5.3, "speed": 3.95, "text": "\"Hello! I'm glad you're interest"}
{"msg_type": "exp", "max_n_beams": 128, "max_beam_len": 32, "min_log_prob": null, "max_budget": 10, "max_branch_width": 32, "gen_rate": 4.63, "gen_rate_micro": 4.51, "gen_speed": 3.27, "gen_speed_micro": 3.161, "t0": 0.1625, "t1": 1.2638, "input_0": 24.0, "input_1": 11.0, "tree_size": 10.0, "tree_w": 2.7, "tree_h": 6.2, "prompt_len": 533.5, "min_CLP": -2.69, "mem_use": 5.53, "exp_time": 128.2}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "model_1": "meta-llama/Llama-2-7b-chat-hf", "temperature": 1.0, "max_n_beams": "128", "max_beam_len": "32", "top_p": 1.0, "max_new_tokens": 32, "max_budget": null, "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-20 09:37:14", "date": "250320", "hostname": "gpu20", "commit": "none", "offload": false, "device": "Tesla V100S-PCIE-32GB"}
