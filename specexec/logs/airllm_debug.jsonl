{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "meta-llama/Llama-2-7b-chat-hf", "model_1": "meta-llama/Llama-2-70b-chat-hf", "temperature": 0.6, "max_n_beams": "10", "max_beam_len": "32", "top_p": 0.9, "max_new_tokens": 32, "max_budget": "100", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-12 18:48:30", "date": "250312", "hostname": "gpu15", "commit": "none", "offload": false, "device": "L40S"}
{"msg_type": "summary", "run": 0, "max_n_beams": 10, "max_beam_len": 32, "branching": null, "max_budget": 100, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 0.6, "draft_temperature": null, "top_p": 0.9, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "meta-llama/Llama-2-7b-chat-hf", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080", "prompt_len": 146, "prompt_text": "What is a Dyson Sphere? [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 1, "new_tokens": 33, "tree_h": 32.0, "tree_w": 4, "tree_size": 100, "t0": 2.11, "t1": 9.07, "tft": 167.2453, "input_0": 314, "input_1": 101, "min_CLP": -4.2, "draft_iters": 32.0, "mem_use": 27.08, "gen_rate": 33.0, "speed": 2.95, "text": " A Dyson Sphere is a hypothetica"}
{"msg_type": "summary", "run": 1, "max_n_beams": 10, "max_beam_len": 32, "branching": null, "max_budget": 100, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 0.6, "draft_temperature": null, "top_p": 0.9, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "meta-llama/Llama-2-7b-chat-hf", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080", "prompt_len": 436, "prompt_text": "heck your facts for me. [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 3, "new_tokens": 32, "tree_h": 23.7, "tree_w": 11, "tree_size": 100, "t0": 1.6333, "t1": 8.81, "tft": 11.4752, "input_0": 306, "input_1": 101, "min_CLP": -4.38, "draft_iters": 24.7, "mem_use": 27.53, "gen_rate": 10.7, "speed": 1.02, "text": " I apologize for the mistake in "}
{"msg_type": "summary", "run": 2, "max_n_beams": 10, "max_beam_len": 32, "branching": null, "max_budget": 100, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 0.6, "draft_temperature": null, "top_p": 0.9, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "meta-llama/Llama-2-7b-chat-hf", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080", "prompt_len": 597, "prompt_text": ". Factorio\n3. Stellaris [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 6, "new_tokens": 41, "tree_h": 22.0, "tree_w": 9, "tree_size": 100, "t0": 1.52, "t1": 8.7983, "tft": 10.8079, "input_0": 368, "input_1": 101, "min_CLP": -4.28, "draft_iters": 23.0, "mem_use": 27.76, "gen_rate": 6.8, "speed": 0.66, "text": " Sure, I can provide some genera"}
{"msg_type": "summary", "run": 3, "max_n_beams": 10, "max_beam_len": 32, "branching": null, "max_budget": 100, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 0.6, "draft_temperature": null, "top_p": 0.9, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "meta-llama/Llama-2-7b-chat-hf", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080", "prompt_len": 181, "prompt_text": "a) when x is user input [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 4, "new_tokens": 62, "tree_h": 28.2, "tree_w": 8, "tree_size": 100, "t0": 1.87, "t1": 8.7925, "tft": 10.086, "input_0": 452, "input_1": 101, "min_CLP": -4.37, "draft_iters": 28.8, "mem_use": 27.13, "gen_rate": 15.5, "speed": 1.45, "text": "\\n Sure, here is a Python script"}
{"msg_type": "summary", "run": 4, "max_n_beams": 10, "max_beam_len": 32, "branching": null, "max_budget": 100, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 0.6, "draft_temperature": null, "top_p": 0.9, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "meta-llama/Llama-2-7b-chat-hf", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080", "prompt_len": 148, "prompt_text": "elp me write my memoir? [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 3, "new_tokens": 42, "tree_h": 32.0, "tree_w": 7, "tree_size": 100, "t0": 2.08, "t1": 8.8633, "tft": 10.9461, "input_0": 468, "input_1": 101, "min_CLP": -4.16, "draft_iters": 32.0, "mem_use": 27.11, "gen_rate": 14.0, "speed": 1.28, "text": "\"\\nOf course, I'd be happy to he"}
{"msg_type": "summary", "run": 5, "max_n_beams": 10, "max_beam_len": 32, "branching": null, "max_budget": 100, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 0.6, "draft_temperature": null, "top_p": 0.9, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "meta-llama/Llama-2-7b-chat-hf", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080", "prompt_len": 632, "prompt_text": "contents of this video. [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 5, "new_tokens": 50, "tree_h": 27.8, "tree_w": 7, "tree_size": 100, "t0": 1.908, "t1": 8.85, "tft": 11.4013, "input_0": 410, "input_1": 101, "min_CLP": -3.73, "draft_iters": 28.6, "mem_use": 27.85, "gen_rate": 10.0, "speed": 0.93, "text": " Sure, here\\'s a bullet list of "}
{"msg_type": "summary", "run": 6, "max_n_beams": 10, "max_beam_len": 32, "branching": null, "max_budget": 100, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 0.6, "draft_temperature": null, "top_p": 0.9, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "meta-llama/Llama-2-7b-chat-hf", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080", "prompt_len": 728, "prompt_text": "ions to enhance images? [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 7, "new_tokens": 34, "tree_h": 17.1, "tree_w": 12, "tree_size": 100, "t0": 1.2257, "t1": 8.8371, "tft": 11.2121, "input_0": 319, "input_1": 101, "min_CLP": -4.28, "draft_iters": 18.4, "mem_use": 27.96, "gen_rate": 4.9, "speed": 0.48, "text": " There are several alternative a"}
{"msg_type": "summary", "run": 7, "max_n_beams": 10, "max_beam_len": 32, "branching": null, "max_budget": 100, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 0.6, "draft_temperature": null, "top_p": 0.9, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "meta-llama/Llama-2-7b-chat-hf", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080", "prompt_len": 1206, "prompt_text": "ge scale manufacturing. [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 4, "new_tokens": 32, "tree_h": 27.8, "tree_w": 7, "tree_size": 100, "t0": 1.9025, "t1": 8.865, "tft": 11.3457, "input_0": 348, "input_1": 101, "min_CLP": -4.04, "draft_iters": 28.2, "mem_use": 28.7, "gen_rate": 8.0, "speed": 0.74, "text": " Sure, I can help you with some "}
{"msg_type": "summary", "run": 8, "max_n_beams": 10, "max_beam_len": 32, "branching": null, "max_budget": 100, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 0.6, "draft_temperature": null, "top_p": 0.9, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "meta-llama/Llama-2-7b-chat-hf", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080", "prompt_len": 699, "prompt_text": "a for all 50 US states. [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 4, "new_tokens": 35, "tree_h": 25.5, "tree_w": 8, "tree_size": 100, "t0": 1.755, "t1": 8.8675, "tft": 10.9955, "input_0": 332, "input_1": 101, "min_CLP": -4.07, "draft_iters": 26.5, "mem_use": 27.92, "gen_rate": 8.8, "speed": 0.82, "text": "\" Sure, here's a list of the leg"}
{"msg_type": "summary", "run": 9, "max_n_beams": 10, "max_beam_len": 32, "branching": null, "max_budget": 100, "max_branch_width": 32, "replacement": false, "verbose": false, "temperature": 0.6, "draft_temperature": null, "top_p": 0.9, "min_log_prob": null, "tree_max_len": 4096, "draft_model_name": "meta-llama/Llama-2-7b-chat-hf", "target_model_name": "/nfs/students/hauh/hub/models--meta-llama--Llama-2-70b-chat-hf/snapshots/e9149a12809580e8602995856f8098ce973d1080", "prompt_len": 562, "prompt_text": "that I could start now? [\\INST]\n", "seed": 0, "max_new_tokens": 32, "iters": 2, "new_tokens": 43, "tree_h": 29.5, "tree_w": 8, "tree_size": 100, "t0": 2.0, "t1": 8.865, "tft": 10.8115, "input_0": 393, "input_1": 101, "min_CLP": -5.16, "draft_iters": 30.0, "mem_use": 27.71, "gen_rate": 21.5, "speed": 1.98, "text": "There are many beginner-friendly"}
{"msg_type": "exp", "max_n_beams": 10, "max_beam_len": 32, "min_log_prob": null, "max_budget": 100, "max_branch_width": 32, "gen_rate": 13.31, "gen_rate_micro": 10.36, "gen_speed": 1.231, "gen_speed_micro": 0.979, "t0": 1.8004, "t1": 8.8619, "input_0": 371.0, "input_1": 101.0, "tree_size": 100.0, "tree_w": 8.1, "tree_h": 26.6, "prompt_len": 533.5, "min_CLP": -4.27, "mem_use": 28.7, "exp_time": 678.52}
