{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "meta-llama/Llama-2-7b-chat-hf", "model_1": "meta-llama/Llama-2-70b-chat-hf", "temperature": 0.6, "max_n_beams": "", "max_beam_len": "32", "top_p": 0.9, "max_new_tokens": 32, "max_budget": "100", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-19 18:51:38", "date": "250319", "hostname": "gpu20", "commit": "none", "offload": true, "device": "Tesla V100-PCIE-32GB", "device_size": 2}
{"msg_type": "config", "gen_type": "SpecExecBase", "model_0": "meta-llama/Llama-2-7b-chat-hf", "model_1": "meta-llama/Llama-2-70b-chat-hf", "temperature": 0.6, "max_n_beams": "", "max_beam_len": "32", "top_p": 0.9, "max_new_tokens": 32, "max_budget": "100", "max_branch_width": "32", "branching": null, "min_log_prob": null, "replacement": false, "n_tests": 10, "seed": 0, "dataset": "oasst", "timestamp": "2025-03-20 09:48:42", "date": "250320", "hostname": "gpu20", "commit": "none", "offload": true, "device": "Tesla V100-PCIE-32GB", "device_size": 2}
